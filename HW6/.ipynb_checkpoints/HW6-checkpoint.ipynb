{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Implementation\n",
    "#### 1. Hyperparameters\n",
    "Hyperparameters included the learning rate - epsilon, the minibatch size, the momentum factor, and the number of hidden layer nodes. I chose a learning schedule of 0.01 that decayed by 75% every 10 epochs, a minibatch size of 50 rows, a momentum factor of 0.1, and 800 hidden layer nodes\n",
    "\n",
    "#### 2. Training Accuracy\n",
    "I set the neural network to stop running at 89% training accuracy validity, which occured after 44 epochs. The final training accuracy was 89.06%\n",
    "\n",
    "#### 3. Validation Accuracy\n",
    "The accuracy of the model on the validation set was 88.42%\n",
    "\n",
    "#### 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(tLoss, tIter, 'b-')\n",
    "plt.plot(mnist_training_sizes, mnist_train_error, 'r-')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Set Size')\n",
    "plt.title('Mnist Error vs Training Set Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as scp\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "####### Neural Network Implementation ######\n",
    "\n",
    "def trainNeuralNetwork(images, labels, epsilon, lam, iterations, batch_size, olddV, olddW, V, W):\n",
    "    loss = []\n",
    "    iteration = []\n",
    "    oldUpdateV = olddV\n",
    "    oldUpdateW = olddW\n",
    "    for i in range(iterations):\n",
    "        minibatch = np.random.randint(low = images.shape[0], size = (batch_size))\n",
    "        image = images[minibatch, :]\n",
    "        label = labels[:, minibatch]\n",
    "        h, z = forward(image, V, W, lam)\n",
    "        dV, dW = backward(image, label, z, V, W, h, lam)\n",
    "        dV = dV / batch_size\n",
    "        dW = dW / batch_size\n",
    "        V = V - ((0.9 * epsilon * dV) + (0.1 * oldUpdateV))\n",
    "        W = W - ((0.9 * epsilon * dW) + (0.1 * oldUpdateW))\n",
    "        oldUpdateV = -((0.9 * epsilon * dV) + (0.1 * oldUpdateV))\n",
    "        oldUpdateW = -((0.9 * epsilon * dW) + (0.1 * oldUpdateW))\n",
    "        if i%50 == 0:\n",
    "            loss.append(calculate_loss(z, label))\n",
    "    return V, W, loss, oldUpdateV, oldUpdateW\n",
    "\n",
    "def forward(im, V, W, lam):\n",
    "    h = np.tanh(V.dot(im.T)) # 200 x 50\n",
    "    h = np.concatenate((h, np.ones((1, h.shape[1]))), axis = 0)\n",
    "    z = scp.special.expit(W.dot(h)) # 26 x 50\n",
    "    return h, z\n",
    "\n",
    "def backward(im, label, z, V, W, h, lam):\n",
    "    gradLz = ((z-label)/(z*(1-z))) # 26 x 50\n",
    "    optim = np.multiply(z, (1 - z)) # 26 x 50\n",
    "    gradLw = np.multiply(gradLz, optim).dot(h.T) # 26 x 201 possible problem\n",
    "    gradLh = W.T.dot(np.multiply(optim, gradLz)) # (201x26).(26x1)*(26x1) = 201x1\n",
    "    gradLh = gradLh[:800,:] # shave off last row of gradLh\n",
    "    gradLv = np.multiply((1/np.square(np.cosh(V.dot(im.T)))).dot(im), np.sum(gradLh, axis = 1, keepdims = True))  #(200x785).(785x1).(1x785) * 200x1 = 200x785\n",
    "    return gradLv, gradLw\n",
    "\n",
    "def predict(im, V, W, lam):\n",
    "    _, pred = forward(im, V, W, lam)\n",
    "    prediction = np.zeros((pred.shape[0], pred.shape[1]))\n",
    "    for i in range(pred.shape[0]):\n",
    "        for j in range(pred.shape[1]):\n",
    "            if pred[i,j] == np.max(pred[:,j]):\n",
    "                prediction[i,j] = 1\n",
    "    return pred, prediction\n",
    "\n",
    "def score(actual, predicted):\n",
    "    return 1 - (np.sum(np.square(predicted - actual))/2)/predicted.shape[1]\n",
    "\n",
    "def calculate_loss(z, y):\n",
    "    return -sum(np.multiply(y, np.log(z)) + np.multiply(1-y, np.log(1-z)))\n",
    "\n",
    "def meta(images, labels, epsilon, lam, epochs, batch_size, V = np.random.randn(200, 785) / np.sqrt(785), W = np.random.randn(26, 201) / np.sqrt(201)):\n",
    "    iterations = int(images.shape[0] / batch_size)\n",
    "    Tloss = []\n",
    "    Titerations = np.arange(0, epochs * iterations, 50)\n",
    "    olddV = 0\n",
    "    olddW = 0\n",
    "    epsilon = epsilon\n",
    "    for i in range(epochs):\n",
    "        V, W, loss, olddV, olddW = trainNeuralNetwork(images, labels, epsilon, lam, iterations, batch_size, olddV, olddW, V, W)\n",
    "        Tloss.extend(loss)\n",
    "        scr = score(labels, predict(images, V, W, lam)[0])\n",
    "        if scr > 0.89:\n",
    "            return V, W, Tloss, Titerations\n",
    "        print(scr)\n",
    "        if (i + 1) % 10 == 0 and epsilon > 0.00134:\n",
    "            epsilon = epsilon * 0.75\n",
    "            \n",
    "    return V, W, Tloss, Titerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### Data preprocessing ######\n",
    "t = loadmat(\"hw6_data_dist/letters_data.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainX = np.array(t.get('train_x'))\n",
    "testX = np.array(t.get('test_x'))\n",
    "trainY = np.array(t.get('train_y'))\n",
    "\n",
    "split = int(np.round(trainX.shape[0] * 0.8))\n",
    "X = np.concatenate((trainX, trainY), axis = 1)\n",
    "np.random.shuffle(X)\n",
    "\n",
    "validX = X[split:, :784]\n",
    "trainX = X[:split, :784]\n",
    "validY = X[split:, 784]\n",
    "trainY = X[:split, 784]\n",
    "\n",
    "### Centering and Normalizing ###\n",
    "trainX = trainX / 255\n",
    "validX = validX / 255\n",
    "testX = testX / 255\n",
    "center = np.mean(trainX, axis = 0)\n",
    "trainX = trainX - center\n",
    "validX = validX - center\n",
    "testX = testX - center\n",
    "\n",
    "trainX = np.concatenate((trainX, np.ones((trainX.shape[0],1))), axis = 1)\n",
    "testX = np.concatenate((testX, np.ones((testX.shape[0], 1))), axis = 1)\n",
    "validX = np.concatenate((validX, np.ones((validX.shape[0], 1))), axis = 1)\n",
    "\n",
    "### 1 hot encoding labels ###\n",
    "trainLab = np.zeros((26, len(trainY)))\n",
    "validLab = np.zeros((26, len(validY)))\n",
    "\n",
    "counter = 0\n",
    "for lab in trainY:\n",
    "    trainLab[lab - 1, counter] = 1\n",
    "    counter += 1\n",
    "\n",
    "counter = 0\n",
    "for lab in validY:\n",
    "    validLab[lab - 1, counter] = 1\n",
    "    counter += 1\n",
    "    \n",
    "trainY = trainLab\n",
    "validY = validLab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Testing\n",
    "#testV, testW = trainNeuralNetwork(trainX, trainY, 0.001, 0, 2000, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.710939248402\n",
      "0.736878770475\n",
      "0.750544807675\n",
      "0.758629018733\n",
      "0.766341497646\n",
      "0.775071193133\n",
      "0.780946892123\n",
      "0.789149406164\n",
      "0.796794854499\n",
      "0.804627844192\n",
      "0.810463192606\n",
      "0.816461488233\n",
      "0.821350316391\n",
      "0.827283047078\n",
      "0.832228842872\n",
      "0.836029696711\n",
      "0.840420279936\n",
      "0.844689826518\n",
      "0.848522346434\n",
      "0.852239505836\n",
      "0.855907648833\n",
      "0.858295914008\n",
      "0.860111390772\n",
      "0.8624910072\n",
      "0.863949605377\n",
      "0.866467964105\n",
      "0.868296732959\n",
      "0.870006150948\n",
      "0.872237288387\n",
      "0.873512244647\n",
      "0.87535572109\n",
      "0.87662975815\n",
      "0.877735562633\n",
      "0.878947622478\n",
      "0.880097781738\n",
      "0.881529050809\n",
      "0.882425454202\n",
      "0.88355798169\n",
      "0.884499748197\n",
      "0.885273419512\n",
      "0.886200755888\n",
      "0.887291791969\n",
      "0.887741081822\n",
      "0.888884319072\n",
      "0.889126501069\n",
      "0.889927314213\n"
     ]
    }
   ],
   "source": [
    "V = np.random.randn(800, 785) / np.sqrt(785)\n",
    "W = np.random.randn(26, 801) / np.sqrt(801)\n",
    "tV, tW, tLoss, tIter = meta(trainX, trainY, 0.01, 0, 1000, 50, V, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88428140017843582"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(validY, predict(validX, tV, tW, 0)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89060680153283056"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(trainY, predict(trainX, tV, tW, 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
